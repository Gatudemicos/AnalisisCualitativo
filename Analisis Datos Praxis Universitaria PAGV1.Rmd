---
title: "Análisis de Datos en Investigación Cualitativa: Práxis Universitaria y Particpación
  Ciudadana"
author: "Dr. José J. Leal"
date: "5 de septiembre de 2021"
output: pdf_document
lang: es-ES
toc: TRUE
---
\pagebreak

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Análisis de Datos recopilados con Protocolos Anécdoticos de Prof. Universitarios, de manera uificada

Limpiar el workspace, consola y fijar a UTF-8
```{r,message=FALSE,warning=FALSE}
rm(list = ls())
cat("\014")
options(encoding = "utf-8")
```

## 1. Cargar Librerias 
```{r,message=FALSE,warning=FALSE}
library(pdftools)
library(dplyr)
library(stopwords)
library(tidytext)
library(stringi)
library(stringr)
library(ggplot2)
library(scales)
library(tidyr)
library(widyr)
library(ggraph)
library(igraph)
library(quanteda)
library(topicmodels)
library(cvTools)
library(wordcloud)
library(RColorBrewer)
```

## 2. Carga y verificación del Protocolo Unificado
```{r,message=FALSE,warning=FALSE}
protocolo <- pdftools::pdf_text("ProtocoloTotal.pdf")
protocolo[1]
length(protocolo)
#head(protocolo) Para ver las 6 primeras observaciones o parrafos
```

## 3. Juntar todas las páginas del Protocolo 
```{r,message=FALSE,warning=FALSE}
protocolo<-paste(protocolo, collapse = '')
length(protocolo)
protocolo[1] 
```

## 4. Estructurar el protocolo como un vector
```{r,message=FALSE,warning=FALSE}
vector = c()
for(i in 1:length(protocolo)){
  temp<-(strsplit(protocolo[[i]], "\\.")[[1]])
  print(temp)
  vector <- c(vector, temp)
}
```

## 5. Convertir a un Dataframe el protocolo por frases
```{r,message=FALSE,warning=FALSE}
frases_protocolo<-as.data.frame(vector)
```
## 6. Limpieza del protocolo y tokenización
```{r,message=FALSE,warning=FALSE}
colnames(frases_protocolo)[1]<-"frase"
# Quitamor espacios de encabezado 
frases_protocolo$frase<-trimws(frases_protocolo$frase, "l") # para la izquierda trimws(frase,'r')
# Convertimos a caracter
frases_protocolo$frase <-as.character(frases_protocolo$frase)
```

## 7. Crear un lexicon de stopwords en español 
```{r,message=FALSE,warning=FALSE}
lexiconSW<-stopwords("es")

# Añadir al stopwords luego de mirar el reviwewords (generado más adelante)
lexiconSW <- append(lexiconSW,c("eeeh","sii", "si", "digo", "bien", "allí", "allá", "van", "eh", "así", "año", "eeh", "van", "ahí","aquí","")) 
lexiconSW <- append(lexiconSW,c("soraida","decir", "san luis", "hace", "decia", "dos", "digamos","paso", "dio", "presentes","manera","quiero","ver"))
lexiconSW <- append(lexiconSW,c("tener","ser", "parte", "tal", "veces", "solamente", "menos","todavia", "ir", "dentro", "cómo", "unas"))
lexiconSW <- append(lexiconSW,c("acá","cuatro", "todas", "decián","decían","entonces","cuál","creo","embargo","pues","través","100","puede"))

# Convertir a un Dataframe
lexiconSW<-as.data.frame(lexiconSW)
names(lexiconSW)<-"word"
lexiconSW$word<-as.character(lexiconSW$word)
```

## 8. Análisis Básicos
```{r,message=FALSE,warning=FALSE}
# Generar un ID para cada frase
df <- tibble::rowid_to_column(frases_protocolo, "ID") 

# Eliminar filas duplicadas basadas en frases
review_words <- df %>%
  distinct(frase, .keep_all = TRUE) %>% 
  unnest_tokens(word, frase, drop = FALSE) %>%
  distinct(ID, word, .keep_all = TRUE) %>%
  anti_join(lexiconSW) %>% 
  filter(str_detect(word, "[^\\d]")) %>% 
  group_by(word) %>% 
  dplyr::mutate(word_total = n()) %>%
  ungroup() 

# Contar palabras resultantes
word_counts <- review_words %>%
  dplyr::count(word, sort = TRUE)

word_counts %>%
  head(40) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "red") +
  scale_y_continuous(labels = comma_format()) +
  coord_flip() +
  labs(title = paste0("Palabras mas utilizadas"),
       subtitle = "Stopwords retiradas",
       x = "Palabra",
       y = "Numero de veces usada")
```

## 9. Generar la Nube de Palabras "WordCloud""
```{r,message=FALSE,warning=FALSE}
df_grouped_V <- review_words %>% group_by(word) %>% count(word) %>%  
  group_by(word) %>% mutate(frecuencia = n/dim(review_words)[1])

windows()
wordcloud(words = df_grouped_V$word, freq = df_grouped_V$frecuencia,
          max.words = 400, random.order = FALSE, rot.per = 0.35,
          colors = brewer.pal(8, "Dark2"))
```

## 10. Generar las relaciones existentes entre frases y palabras por Bigramas, Trigrams, ó n-grams
```{r,message=FALSE,warning=FALSE}
review_bigrams <- df %>%
  unnest_tokens(bigram, frase, token = "ngrams", n = 2) # separamos token 2 - grams
bigrams_separated <- review_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") # separamos word por bigrama, o n-grama
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% lexiconSW$word) %>%
  filter(!word2 %in% lexiconSW$word) %>% # eliminamos  stop words por bigrama
bigram_counts <- bigrams_filtered %>% 
  dplyr::count(word1, word2, sort = TRUE) # contamos la cantidad de words por bigrama
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ") # count bigrams cleaning
bigrams_united %>%
  dplyr::count(bigram, sort = TRUE)
```

## 11. Visualizando el Mapa de Relaciones
```{r,message=FALSE,warning=FALSE}
review_subject <- df %>% 
  unnest_tokens(word, frase) %>% 
  anti_join(lexiconSW)
my_stopwords <- data_frame(word = c(as.character(1:10)))
review_subject <- review_subject %>% 
  anti_join(my_stopwords)
title_word_pairs <- review_subject %>% 
  pairwise_count(word, ID, sort = TRUE, upper = FALSE)

# Nos generamos el listado de bigramas, n-gramas, EN ESTA PRUEBA con N=3 luego de revisado el "bigram_counts"
#de usar un N > al de relaciones observadas arroja ERROR 

windows()
listadoBigramas<-title_word_pairs[which(title_word_pairs$n>3),]
set.seed(2)
title_word_pairs %>%
  filter(n >= 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "orange3") +
  geom_node_point(size = 3) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  ggtitle('Bigramas Protocolos Anecdoticos Unificados') 
```


